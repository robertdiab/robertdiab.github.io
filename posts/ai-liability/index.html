<!DOCTYPE html>
<html lang="en-us"><head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-GXV01KRXE5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GXV01KRXE5');
</script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
<meta name="description" content="Generative AI and the Limits of Liability in Canadian Law">


<meta name="keywords" content="[AI liability risk harm Canada]">


<link rel="stylesheet" media="screen and (min-width: 441px)" href="/css/widescreen.css">

<link rel="stylesheet" media="screen and (max-width: 440px)" href="/css/smallscreen.css">

  <title>When AI Turns Deadly: Are Model Makers Responsible? | Robert Diab</title>

  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon/favicon-16x16.png">
  <link rel="manifest" href="/images/favicon/site.webmanifest">
  <link rel="mask-icon" href="/images/favicon/safari-pinned-tab.svg">
  <link rel="shortcut icon" href="/images/favicon/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-config" content="/images/favicon/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">


</head>
<body>
      <div class="hdsection">

<div class="container">
<header>









<div class="sitelogo-field">
<a href=" / "><h1>Robert Diab</h1></a>
</div></span>














<nav>
    <ul>
        <li><a href="/commentary" style="">Commentary</a><p1>|</p1></li>

        <li-2><a href="/publications" style="">Papers/Books</a></li-2>


    </ul>
</nav>



</header>
</div>
</div>

      <div class="sidenav">








<span class="smallnav"><p>Professor of law at Thompson Rivers University, writing about AI and online freedoms. More <a href="https://www.tru.ca/law/faculty-staff/faculty/robert-diab.html" style="">here</a></p></span>

<span class="smallnav"><p>Follow on <a href="https://robertdiab.substack.com" style="">Substack</a> & <a href="https://bsky.app/profile/robertdiab.bsky.social" style="">Bluesky</a></p></span>

<span class="navtitle">Recent posts</span><br>
<span class="smallnav">
<div class=recent-posts>

  

<a href="https://www.robertdiab.ca/posts/authorship-ai/">Authorship After AI<div class=space> </div></a><br>

  

<a href="https://www.robertdiab.ca/posts/ai-liability/">When AI Turns Deadly: Are Model Makers Responsible?<div class=space> </div></a><br>

  

  </div>
</span>

<span class="navtitle-earlier">Earlier posts</span><br>
<span class="earlier-posts"><a href="/posts" style="">By date</a></span>
<br>
<span class="earlier-posts"><a href="/tags/" style="">By topic</a></span>
<br>
<span class="earlier-posts"><a href="/index.xml" style="">RSS Feed</a></span><br>
</div>

        <div class="content">

<div class=post-content>
<div class=post-title> When AI Turns Deadly: Are Model Makers Responsible?</div>


          <span class="post-date">
            Aug 30, 2025
          </span>

        
<p><p>This week, parents of Adam Raine, a California teen who committed suicide in April after a lengthy interaction with GPT-4o, <a href="https://www.theguardian.com/technology/2025/aug/27/chatgpt-scrutiny-family-teen-killed-himself-sue-open-ai">filed a lawsuit</a> against OpenAI and its CEO, Sam Altman. The case follows a <a href="https://www.nytimes.com/2024/10/23/technology/characterai-lawsuit-teen-suicide.html">suit</a> brought in late 2024 by the parents of a Florida teen, Sewell Setzer, who took his own life after engaging with a Character.AI chatbot impersonating Daenerys Targaryen from Game of Thrones.</p>
<p>In early August, ChatGPT was also implicated in a <a href="https://www.wsj.com/tech/ai/chatgpt-ai-stein-erik-soelberg-murder-suicide-6b67dbfb">murder-suicide in Connecticut</a> involving 56-year-old tech worker Stein-Erik Soelberg, who had a history of mental illness. Although the chatbot did not suggest that he murder his mother, it appears to have fueled Soelberg’s paranoid delusions, which led him to do so.</p>
<p>OpenAI and other companies have been quick to respond with <a href="https://openai.com/index/helping-people-when-they-need-it-most/">blog posts</a> and <a href="https://openai.com/index/openai-anthropic-safety-evaluation/">press releases</a> outlining steps they are taking to mitigate risks from misuse of their models.</p>
<p>This raises a larger question left unanswered in Canada after the <a href="https://ised-isde.canada.ca/site/innovation-better-canada/en/artificial-intelligence-and-data-act">Artificial Intelligence and Data Act</a> died on the order paper in early 2025, when the last Parliament ended: what guardrails exist in Canadian law to govern the harmful uses of generative AI?</p>
<p>Like the United States, Canada has no national or provincial legislation designed to impose liability on AI companies for harms caused by their products. The European Union passed an <a href="https://artificialintelligenceact.eu/ai-act-explorer/">AI Act</a> in 2024 that does impose liability for harmful AI systems.</p>
<p>But in both the EU law and the Canadian bill that was abandoned, there is a notable flaw in how liability is conceived.</p>
<p>I explored this in <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4680927">a paper I wrote</a> in late 2023, surveying early reports of harmful uses of language models (a suicide in Belgium, help with bomb-making, and other cases).</p>
<p>My article garnered some interest on SSRN but only recently appeared in print (it was <a href="https://commons.allard.ubc.ca/cgi/viewcontent.cgi?article=1372&amp;context=ubclawreview">published</a> this month). The core argument was this:</p>
<blockquote>
<p>Both [the European and Canadian AI] bills are premised on the ability to quantify in advance and to a reasonable degree the nature and extent of the risk a system poses. This paper canvases evidence that raises doubt about whether providers or auditors have this ability. It argues that while providers can take measures to mitigate risk to some degree, remaining risks are substantial, but difficult to quantify, and may persist for the foreseeable future due to the intractable problem of novel methods of jailbreaking and limits to model interpretability.</p>
</blockquote>
<p>The problem remains unresolved.</p>
<h3 id="the-only-guardrails-at-the-moment">The only guardrails at the moment</h3>
<p>The only mechanisms in Canada and the US for holding AI companies liable are laws on product liability, negligence, and wrongful death.</p>
<p>Parents in both the California and Florida cases are suing the model makers (OpenAI and Character.AI, respectively) for wrongful death, a statutory cause of action that allows family members of the deceased to sue for damages including funeral expenses, mental anguish, loss of future financial support, and companionship. Plaintiffs must show the defendant’s negligence or intentional misconduct caused the death.</p>
<p>Here, parents allege that chatbot makers were negligent in product design and failed to provide adequate warnings about risks.</p>
<p>Canadian law works in a similar way. Provinces allow wrongful death suits for a wrongful act. Damage awards in Canada are much smaller than in the US and mostly limited to quantifiable losses. But plaintiffs can also claim that a model maker was negligent in offering a harmful product, or that it was defective or lacked adequate warnings.</p>
<p>At the heart of negligence and product liability is the same question: what steps should OpenAI, Anthropic, or Google reasonably have taken to avoid harm?</p>
<p>Put another way, in making chatbots available, companies clearly owe users a duty of care. The product carries risks, and harm to users is foreseeable.</p>
<p>The key question, though, is: what is the standard of care?</p>
<p>When can OpenAI and others be said to have done enough—or not enough—to avoid harm? If the standard is “reasonably safe” rather than “absolutely safe,” when is that threshold met? And can it even be met, given the nature of these systems?</p>
<p>No one knows. But OpenAI and others are taking—and <a href="https://openai.com/index/helping-people-when-they-need-it-most/">publicizing</a>—all the steps one might predict a tort lawyer would advise them to take.</p>
<p>OpenAI admits its risk-detection mechanisms work better in shorter conversations and degrade as conversations lengthen. It is working to improve performance in longer chats.</p>
<p>It is also improving detection across different types of harmful conversations, from suicidal to criminal. It has announced plans for parental controls to let parents monitor their child’s activity, and is rolling out systems to route some conversations to human overseers who can terminate the chat and lock the user out of further access.</p>
<p>Whether these steps will be deemed sufficient—enough to absolve OpenAI and others of liability—remains to be seen.</p>
<p>Much may depend on how a model was misused, what jailbreak was employed, and whether that misuse was foreseeable.</p>
<p>In a broader sense, it is worth keeping perspective on AI risks. As tragic as these cases are, hundreds of millions of people use these tools daily, and many find them beneficial. But there are, inevitably, many ways to misuse them.</p>
</p>







	<nav class="page-nav">


   
   <a class="btn btn-default" href="https://www.robertdiab.ca/posts/authorship-ai/">
     <b>Next:</b> <span>Authorship After AI</span>
   </a>
   

<br>

   
   <a class="btn btn-default" href="https://www.robertdiab.ca/posts/c2backgrounder/">
     <b>Previous:</b>
     <span>Bill C-2 Backgrounder - the missing manual!</span>
   </a>
   


 </nav>

</div>


<div class=bottomhomelinks>
<br>
<a href="/">Back to main page</a> | <a href="/posts">Archive</a>
</div>



        </div>

    </body>
</html>
